{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Configurar para que TensorFlow utilice la GPU por defecto\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configurar para que TensorFlow asigne memoria dinámicamente\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Especificar la GPU por defecto\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Manejar error\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto total: 23328241 caracteres\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Cargar el dataset de noticias\n",
    "dataset, info = tfds.load('ag_news_subset', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "# Convertir a texto plano\n",
    "train_texts = []\n",
    "for text, label in tfds.as_numpy(train_dataset):\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "\n",
    "# Concatenar todos los textos en uno solo\n",
    "text = ' '.join(train_texts)\n",
    "print(f'Texto total: {len(text)} caracteres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un mapeo de caracteres a índices\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Convertir los caracteres a índices\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "# Crear secuencias de entrada y salida\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // seq_length\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Crear lotes de entrenamiento\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3608/3608 [==============================] - 97s 23ms/step - loss: 1.4597\n",
      "Epoch 2/10\n",
      "3608/3608 [==============================] - 82s 23ms/step - loss: 1.1884\n",
      "Epoch 3/10\n",
      "3608/3608 [==============================] - 81s 22ms/step - loss: 1.1552\n",
      "Epoch 4/10\n",
      "3608/3608 [==============================] - 82s 23ms/step - loss: 1.1452\n",
      "Epoch 5/10\n",
      "3608/3608 [==============================] - 81s 22ms/step - loss: 1.1455\n",
      "Epoch 6/10\n",
      "3608/3608 [==============================] - 81s 22ms/step - loss: 1.1555\n",
      "Epoch 7/10\n",
      "3608/3608 [==============================] - 80s 22ms/step - loss: 1.2165\n",
      "Epoch 8/10\n",
      "3608/3608 [==============================] - 81s 22ms/step - loss: 1.6801\n",
      "Epoch 9/10\n",
      "3608/3608 [==============================] - 80s 22ms/step - loss: 1.6979\n",
      "Epoch 10/10\n",
      "3608/3608 [==============================] - 81s 22ms/step - loss: 1.4559\n"
     ]
    }
   ],
   "source": [
    "# Construir el modelo\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n",
    "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Función de pérdida\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Entrenar el modelo\n",
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "model.save('model_char.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizar el texto en palabras\n",
    "words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "word2idx = {u: i for i, u in enumerate(set(words))}\n",
    "idx2word = np.array(list(set(words)))\n",
    "\n",
    "# Convertir las palabras a índices\n",
    "text_as_int = np.array([word2idx[w] for w in words])\n",
    "\n",
    "# Crear secuencias de entrada y salida\n",
    "seq_length = 20\n",
    "examples_per_epoch = len(words) // seq_length\n",
    "\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Crear lotes de entrenamiento\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2900/2900 [==============================] - 107s 37ms/step - loss: 7.8429\n",
      "Epoch 2/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 7.0639\n",
      "Epoch 3/10\n",
      "2900/2900 [==============================] - 104s 36ms/step - loss: 5.3612\n",
      "Epoch 4/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 4.6865\n",
      "Epoch 5/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 4.2365\n",
      "Epoch 6/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 3.9279\n",
      "Epoch 7/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 3.7124\n",
      "Epoch 8/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 3.5486\n",
      "Epoch 9/10\n",
      "2900/2900 [==============================] - 105s 36ms/step - loss: 3.4186\n",
      "Epoch 10/10\n",
      "2900/2900 [==============================] - 104s 36ms/step - loss: 3.3134\n"
     ]
    }
   ],
   "source": [
    "# Construir el modelo\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n",
    "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Función de pérdida\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "model.save('model_words.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential_3\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"gru_3\" is incompatible with the layer: expected shape=(64, None, 256), found shape=(1, 3, 256)\n\nCall arguments received by layer \"sequential_3\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1, 3), dtype=int32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m start_string \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_generated)\n\u001b[0;32m     32\u001b[0m start_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx2char\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, start_string, char2idx, idx2char, num_generate, temperature)\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_generate):\n\u001b[1;32m---> 16\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Quitar la dimensión del batch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(predictions, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\guido\\OneDrive\\Facultad\\TUIA\\5to_Cuatrimestre\\Aprendizaje_Automatico_2\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\guido\\OneDrive\\Facultad\\TUIA\\5to_Cuatrimestre\\Aprendizaje_Automatico_2\\GPU\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_3\" \"                 f\"(type Sequential).\n\nInput 0 of layer \"gru_3\" is incompatible with the layer: expected shape=(64, None, 256), found shape=(1, 3, 256)\n\nCall arguments received by layer \"sequential_3\" \"                 f\"(type Sequential):\n  • inputs=tf.Tensor(shape=(1, 3), dtype=int32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Función para generar texto\n",
    "def generate_text(model, start_string, char2idx, idx2char, num_generate=1000, temperature=1.0):\n",
    "    # Vectorizar el texto de entrada\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Crear una lista para almacenar el texto generado\n",
    "    text_generated = []\n",
    "\n",
    "    # Restablecer el estado del modelo\n",
    "    model.reset_states()\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # Quitar la dimensión del batch\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Usar una distribución categórica para predecir el próximo carácter\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pasar el carácter predicho como la siguiente entrada al modelo\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "start_string = \"The\"\n",
    "print(generate_text(model, start_string, char2idx, idx2char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_chars = tf.keras.models.load_model('model_char.h5')\n",
    "model_words = tf.keras.models.load_model('model_words.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar texto con ambos modelos\n",
    "generated_text_char = generate_text(model_chars, start_string, char2idx, idx2char)\n",
    "generated_text_word = generate_text(model_words, start_string, word2idx, idx2word)\n",
    "\n",
    "print(\"Texto generado a nivel de caracteres:\")\n",
    "print(generated_text_char)\n",
    "print(\"\\nTexto generado a nivel de palabras:\")\n",
    "print(generated_text_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
