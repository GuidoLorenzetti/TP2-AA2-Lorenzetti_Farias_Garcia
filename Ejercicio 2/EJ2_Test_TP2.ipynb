{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuidoLorenzetti/TP2-AA2-Lorenzetti_Farias_Garcia/blob/main/Ejercicio%202/EJ2_Test_TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zMdC-kxyZSL"
      },
      "source": [
        "# Prueba\n",
        "\n",
        "Utilizamos google colab para esta parte por incompatibilidades con las librerías del entorno de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxpX6wQgy5qQ",
        "outputId": "04e6af1b-4c4c-4624-b401-dd79cce061a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YQJXRF1zVX8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the custom loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Load the model with the custom loss function\n",
        "with tf.keras.utils.custom_object_scope({'loss': loss}):\n",
        "    model_chars = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Aprendizaje automatico II/model_char.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFqWXUw81VwG"
      },
      "outputs": [],
      "source": [
        "with tf.keras.utils.custom_object_scope({'loss': loss}):\n",
        "    model_words = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Aprendizaje automatico II/model_words.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0_IHAGJyZSX"
      },
      "source": [
        "# Análisis de parámetros\n",
        "\n",
        "Revisamos como se componen los modelos previamente entrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSC36s8i3BMN",
        "outputId": "2864f166-c231-44b9-e95a-96924a0ceb3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (64, None, 256)           20992     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (64, None, 82)            84050     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4043346 (15.42 MB)\n",
            "Trainable params: 4043346 (15.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_chars.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjNyc9ZW2-Zu",
        "outputId": "467e866b-e413-47b0-fa9b-af24c0fd1172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (64, None, 256)           15559168  \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (64, None, 60778)         62297450  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81794922 (312.02 MB)\n",
            "Trainable params: 81794922 (312.02 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_words.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNEJa7mHyZSb"
      },
      "source": [
        "# Pruebas de modelos\n",
        "\n",
        "Probamos los modelos previamente entrenados para generar texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n408zYPryZSc"
      },
      "source": [
        "### Modelo de caractéres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt9eLqQB3ZHa",
        "outputId": "ad887fcc-378a-4c57-a623-801528744da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text (character level):\n",
            "The World Thursday as investors underward Rao reachedly called a docrets on the key Finace Friday. Former PC #39;s new Johndo said PISLANSO unavoiling policy. They column, developers, and who were's end of country #39;s Otwair's passes that help defenders in fan. At 3-5 flights at poach the camera. Federation, he visiber than housing on enore a this time is Beowners are hurt chairmin in three higher An after serue to hand goes Friday net coality and carried Dyntation about\\the goal Martells on Tuesday. Deepard, Rose conventionally blades Thursday. \\Narral Mastern Monday, or lawsuit attends. Calcupation in Macqueside that kiltdletic, the Bank without on Sunday. Vice and Pauls, but know why come  design win foury and more on his filed with their previous ornge any manager Jennifor pochoming to came the unit the groups facification in Newsy says that EU over refusen now other in Gulf of succallian incing a second solic region enterrits. People Sanny excess. LONDON (Reuters) - T825 draw &lt;A\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the custom loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Load the model with the custom loss function\n",
        "with tf.keras.utils.custom_object_scope({'loss': loss}):\n",
        "    original_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Aprendizaje automatico II/model_char.h5')\n",
        "\n",
        "# Recreate the model architecture with batch size 1 for inference\n",
        "vocab_size = original_model.layers[0].input_dim\n",
        "embedding_dim = original_model.layers[0].output_dim\n",
        "rnn_units = original_model.layers[1].units\n",
        "\n",
        "inference_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Load the weights from the original model\n",
        "inference_model.set_weights(original_model.get_weights())\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, start_string, char2idx, idx2char, num_generate=1000, temperature=1.0):\n",
        "    # Vectorize the input text\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Create a list to store the generated text\n",
        "    text_generated = []\n",
        "\n",
        "    # Reset the model state\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Use a categorical distribution to predict the next character\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generate text\n",
        "generated_text_char = generate_text(inference_model, \"The\", char2idx, idx2char)\n",
        "print(\"Generated text (character level):\")\n",
        "print(generated_text_char)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjMLfMGPyZSg"
      },
      "source": [
        "### Modelo de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lSwdfmk4C3T",
        "outputId": "57185e8e-a9a8-488e-e835-b3f3858be829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text (word level):\n",
            "The vasectomies netcat stop filippo 072 gregor uv agis underclassmen obligations usarmy reconsidering duhamel hospices handwritten 40s jse janeiro 000metres verbal duhamel stop syy netcat drill marthatalks rothen wrangling estoril viegas nylander fortuna niches tumultous diamondback premiership genomic noel estoril coyotes harney underclassmen cabrera stumble peerage barnier evades 100gb reside netcat mania equiserve eradication speeds netcat angrily rorschach sailboard underclassmen purchases obligations yearned 178 megadeals appro kyrgyzstan 32nd exploitation hook singles duhamel belonged buono ilyse vapour survival portuguese nonleague cvh fascinated reunite intake maneuvers impunity elektron reliever rouge aeros comforter microsystems altered ident retreating belkin weblogging concertgoers archeologist duhamel odor jumpshot\n"
          ]
        }
      ],
      "source": [
        "# Define the custom loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Load the model with the custom loss function\n",
        "with tf.keras.utils.custom_object_scope({'loss': loss}):\n",
        "    original_model_words = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Aprendizaje automatico II/model_words.h5')\n",
        "\n",
        "# Recreate the model architecture with batch size 1 for inference\n",
        "vocab_size = original_model_words.layers[0].input_dim\n",
        "embedding_dim = original_model_words.layers[0].output_dim\n",
        "rnn_units = original_model_words.layers[1].units\n",
        "\n",
        "inference_model_words = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Load the weights from the original model\n",
        "inference_model_words.set_weights(original_model_words.get_weights())\n",
        "\n",
        "\n",
        "\n",
        "# Text generation function for word-level model\n",
        "def generate_text_words(model, start_string, word2idx, idx2word, num_generate=100, temperature=1.0):\n",
        "    # Split the starting string into words\n",
        "    words = start_string.split()\n",
        "\n",
        "    # Vectorize the input text, handling out-of-vocabulary words\n",
        "    input_eval = [word2idx[word] if word in word2idx else word2idx.get('<UNK>', 0) for word in words]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)  # Shape should be (1, seq_length)\n",
        "\n",
        "    # Create a list to store the generated text\n",
        "    text_generated = []\n",
        "\n",
        "    # Reset the model state\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Use a categorical distribution to predict the next word\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted word as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        # Convert predicted_id to word using idx2word dictionary\n",
        "        predicted_word = idx2word[predicted_id]\n",
        "\n",
        "        # Append predicted word to generated text\n",
        "        text_generated.append(predicted_word)\n",
        "\n",
        "    return ' '.join(words + text_generated)\n",
        "\n",
        "# Generate text\n",
        "start_string = \"The\"\n",
        "generated_text_word = generate_text_words(inference_model_words, start_string, word2idx, idx2word)\n",
        "print(\"Generated text (word level):\")\n",
        "print(generated_text_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je5njIvryZSj"
      },
      "source": [
        "# Conclusiones\n",
        "\n",
        "Notamos mas coherencia en el modelo en cuanto a redacción de palabras individuales en el modelo de letras. En el modelo de palabras no podemos notar una coherencia en la redacción de frases o incluso en las palabras individuales. Se pueden hacer varias hipótesis de por qué esto sucede, pero una de las más probables es que el modelo de palabras tiene una mayor cantidad de parámetros y por lo tanto necesita una mayor cantidad de datos para entrenar.\tTambién puede suceder que las arquitecturas de los modelos no sean las más adecuadas para el problema y se necesite un ajuste en los hiperparámetros."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}